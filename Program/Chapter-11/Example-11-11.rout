Welcome to R on cspark Computer!

> # Example 1: Math Proficiency with one predictor (X2). Page 441
> # NOTE: Typo in Figure 11.5 on Page 442. (X3 should read X2).
> 
> # Page 443
> Data=read.table("https://raw.githubusercontent.com/AppliedStat/LM/master/CH11TA04.txt")
> y  = Data[,2]
> X2 = Data[,4]
> 
> #
> X2bar = mean(X2); x2 = X2 - X2bar  # de-meaned
> 
> # OLS 
> LM0 = lm( y~x2 + I(x2^2) )
> e0 = resid(LM0)
> u0 = e0 / mad(e0)
> 
> #
> weight.huber  <- function(x, k=1.345) { pmin(1, k/abs(x) ) }
> 
> # WLS: 1st iteration
> w1 = weight.huber(u0)
> LM1 = lm( y~x2 + I(x2^2), weights=w1)
> e1 = resid(LM1)
> 
> # WLS: 2nd iteration
> u1= e1 / mad(e1) 
> w2 = weight.huber(u1)
> LM2 = lm( y~x2 + I(x2^2), weights=w2)
> e2 = resid(LM2)
> 
> # WLS: 3rd iteration
> u2= e2 / mad(e2) 
> w3 = weight.huber(u2)
> LM3 = lm( y~x2 + I(x2^2), weights=w3)
> e3 = resid(LM3)
> 
> # WLS: 4th iteration
> u3= e3 / mad(e3)
> w4 = weight.huber(u3)
> LM4 = lm( y~x2 + I(x2^2), weights=w4)
> e4 = resid(LM4)
> 
> # WLS: 5th iteration
> u4= e4 / mad(e4)
> w5 = weight.huber(u4)
> LM5 = lm( y~x2 + I(x2^2), weights=w5)
> e5 = resid(LM5)
> 
> # WLS: 6th iteration
> u5= e5 / mad(e5)
> w6 = weight.huber(u5)
> LM6 = lm( y~x2 + I(x2^2), weights=w6)
> e6 = resid(LM6)
> 
> # WLS: 7th iteration
> u6= e6 / mad(e6)
> w7 = weight.huber(u6)
> LM7 = lm( y~x2 + I(x2^2), weights=w7)
> e7 = resid(LM7)
> 
> # Table 11.5 (Page 444)
> round(cbind(e0,u0, w1,e1, w2, e2, w7,e7),4)
         e0      u0     w1       e1     w2       e2     w7       e7
1   -2.4109 -0.5164 1.0000  -3.7542 1.0000  -4.0354 1.0000  -4.1269
2   10.5724  2.2646 0.5939   8.4297 0.7152   7.4848 0.8601   6.7698
3    3.0454  0.6523 1.0000   1.5411 1.0000   1.1559 1.0000   0.9731
4   10.3104  2.2085 0.6090   7.3822 0.8166   5.4138 1.0000   3.6583
5   -1.2395 -0.2655 1.0000  -1.4402 1.0000  -1.3970 1.0000  -1.3160
6   -0.7342 -0.1573 1.0000  -0.7696 1.0000  -0.7376 1.0000  -0.6986
7   -2.6394 -0.5654 1.0000  -3.1694 1.0000  -3.1469 1.0000  -3.0318
8  -20.6282 -4.4186 0.3044 -22.2929 0.2704 -22.7964 0.2526 -23.0873
9    6.5724  1.4078 0.9554   4.4297 1.0000   3.4848 1.0000   2.7698
10   0.2871  0.0615 1.0000  -0.7325 1.0000  -0.8491 1.0000  -0.8079
11 -14.8358 -3.1779 0.4232 -18.3824 0.3280 -21.4286 0.2402 -24.3167
12   5.0224  1.0758 1.0000   2.2502 1.0000   0.5153 1.0000  -0.9987
13   6.1255  1.3121 1.0000   5.7598 1.0000   5.7999 0.9875   5.9063
14  -1.5341 -0.3286 1.0000  -2.2278 1.0000  -2.2373 1.0000  -2.1301
15   1.1255  0.2411 1.0000   0.7598 1.0000   0.7999 1.0000   0.9063
16   1.8868  0.4042 1.0000   2.1841 1.0000   2.1504 1.0000   2.0553
17   1.5891  0.3404 1.0000   0.2458 1.0000  -0.0354 1.0000  -0.1269
18  -5.6282 -1.2056 1.0000  -7.2929 0.8266  -7.7964 0.7215  -8.0873
19  -3.6394 -0.7796 1.0000  -4.1694 1.0000  -4.1469 1.0000  -4.0318
20  -1.8745 -0.4015 1.0000  -2.2402 1.0000  -2.2001 1.0000  -2.0937
21  -0.1132 -0.0242 1.0000   0.1841 1.0000   0.1504 1.0000   0.0553
22   3.8868  0.8326 1.0000   4.1841 1.0000   4.1504 1.0000   4.0553
23  -0.1132 -0.0242 1.0000   0.1841 1.0000   0.1504 1.0000   0.0553
24  -3.1132 -0.6669 1.0000  -2.8159 1.0000  -2.8496 1.0000  -2.9447
25   3.1255  0.6695 1.0000   2.7598 1.0000   2.7999 1.0000   2.9063
26   8.3796  1.7949 0.7493   6.0787 0.9918   4.9579 1.0000   4.0681
27   5.0030  1.0717 1.0000   3.8213 1.0000   3.6296 1.0000   3.6128
28  -4.4109 -0.9448 1.0000  -5.7542 1.0000  -6.0354 0.9521  -6.1269
29  -1.0114 -0.2166 1.0000  -0.3793 1.0000  -0.5362 1.0000  -0.8322
30  -1.8745 -0.4015 1.0000  -2.2402 1.0000  -2.2001 1.0000  -2.0937
31   8.5891  1.8398 0.7311   7.2458 0.8320   6.9646 0.8482   6.8731
32   9.4659  2.0276 0.6633   8.7722 0.6872   8.7627 0.6575   8.8699
33  -4.7342 -1.0141 1.0000  -4.7696 1.0000  -4.7376 1.0000  -4.6986
34   2.2871  0.4899 1.0000   1.2675 1.0000   1.1509 1.0000   1.1921
35  11.6046  2.4857 0.5411   8.9889 0.6707   7.4732 0.9403   6.1839
36 -33.6282 -7.2032 0.1867 -35.2929 0.1708 -35.7964 0.1616 -36.0873
37   2.4659  0.5282 1.0000   1.7722 1.0000   1.7627 1.0000   1.8699
38  -1.7129 -0.3669 1.0000  -2.7325 1.0000  -2.8491 1.0000  -2.8079
39   3.2658  0.6995 1.0000   3.2304 1.0000   3.2624 1.0000   3.3014
40   1.2658  0.2711 1.0000   1.2304 1.0000   1.2624 1.0000   1.3014
> 
> 
> # Using rlm() function, which is slightly different from the above.
> library(MASS)
> RLM1 = rlm(y~x2 + I(x2^2), method="M", scale.est="MAD", k2=1.345, maxit=1 )
> RLM1
Call:
rlm(formula = y ~ x2 + I(x2^2), scale.est = "MAD", k2 = 1.345, 
    maxit = 1, method = "M")
Ran 1 iterations without convergence

Coefficients:
 (Intercept)           x2      I(x2^2) 
259.38160409   1.67081807   0.06476101 

Degrees of freedom: 40 total; 37 residual
Scale estimate: 4.62 
> RLM7 = rlm(y~x2 + I(x2^2), method="M", scale.est="MAD", k2=1.345, maxit=7 )
> RLM7
Call:
rlm(formula = y ~ x2 + I(x2^2), scale.est = "MAD", k2 = 1.345, 
    maxit = 7, method = "M")
Ran 7 iterations without convergence

Coefficients:
 (Intercept)           x2      I(x2^2) 
259.42100205   1.56491518   0.08016681 

Degrees of freedom: 40 total; 37 residual
Scale estimate: 4.34 
> RLM  = rlm(y~x2 + I(x2^2))
> RLM
Call:
rlm(formula = y ~ x2 + I(x2^2))
Converged in 10 iterations

Coefficients:
 (Intercept)           x2      I(x2^2) 
259.42112605   1.56460704   0.08021299 

Degrees of freedom: 40 total; 37 residual
Scale estimate: 4.34 
> 
> 
> # Plot 
> # Scatter plot
> postscript(file="Figure-11-5.eps", width=5, height=5)
> plot(X2, y) 
> legend(63,285, bty="n", lty=c(1,2,2), col=c("blue","black","red"),
+        legend=c("rlm", "WLS", "OLS") )
> # From LM0 (OLS)
> curve( 258.43557+1.83272*(x-X2bar)+0.06491*(x-X2bar)^2,add=TRUE, col="red", lty=2) 
> 
> # From LM1 (WLS after 1st iteration)
> curve( 259.39021+ 1.67011*(x-X2bar)+0.06463*(x-X2bar)^2,add=TRUE, lty=2)
> 
> # From RLM (MASS library) # sams as WLS after 10th iteration.;
> curve( 259.421126+ 1.5646*(x-X2bar)+0.08021299*(x-X2bar)^2,add=TRUE, col="blue")
> 
> 

   Goodbye!

